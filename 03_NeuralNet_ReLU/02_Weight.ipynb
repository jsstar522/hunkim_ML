{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Weight.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsstar522/hunkim_ML/blob/master/03_NeuralNet_ReLU/02_Weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5kH4YSGmz3_",
        "colab_type": "text"
      },
      "source": [
        "# Weight 설정하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKEvXZ1Ipzxe",
        "colab_type": "text"
      },
      "source": [
        "## Weight는 normal_random으로 주는 것이 좋을까?\n",
        "기존에는 X * W = Y의 식을 토대로 input값이 주어지면 무작위의 weight값을 준 뒤 output을 도출하고, 실제 Y값과 비교되는 cost를 뽑아낸 뒤, backpropagation을 통해 **weight 값을 조절해 나아가는 방식**으로 진행 됐습니다. 하지만 weight 값을 random으로 주게 된다면 적절하지 못한 weight이 걸렸을 때, Vanishing Gradient가 발생할 확률이 높습니다. 0과 같은 값이 결정되면 학습조차 안되죠. 그래서 **좋은 weight값을 가지고 학습을 시작하는 `Deep Belief Network`가 탄생합니다.**\n",
        "\n",
        "## Deep Belief Network\n",
        "기존의 Multi Layer Network에서는 weight 값을 모두 random으로 준 뒤, 최종적으로 나온 output값을 이용해 Backpropagation으로 weight값을 **역조정으로 조절** 했습니다. `Deep Belief Network`는 반대입니다. `Deep Belief Network`는 input 값 (visible node) 하나만으로 학습하기 좋은 첫번째 Layer의 weight을 만들어내고 그다음 Layer의 Weight도 만들어내고...의 과정을 반복합니다. **이는 weight 값과 Y값 (다음 layer에서 input값이 되는 값)을 구해야 한다는 말과 같습니다.** W와 Y를 모르는 상태에서 input값만 가지고 W, Y를 결정하는 건 멈추지 않고 구구단을 외는 것과 같습니다. 이를 해결하기 위해서 `RBM`라는 알고리즘을 사용합니다. \n",
        "\n",
        "## Energy Based Model\n",
        "쉽게 보면 input * weight = output의 형태를 띄고 있는 hypothesis이지만 지금의 경우에는 weight값과 output값을 알지 못합니다. **하지만 우리는 weight값이 output값에 어떻게 영향을 미치는지 알고 있습니다. 그래서 `분포`의 개념을 사용해야 합니다.** 우리는 열역학 제 2의 법칙에 대응대는 확률분포를 가져올 것입니다. \n",
        "\n",
        "**`열역학 제 2법칙`은 고립계에서 열적 평형상태가 아닐 경우, 엔트로피는 항상 증가한다는 법칙입니다.** 이 말은 다시말해, **열의 이동은 에너지가 높은 불균형상태**에서 **에너지가 낮은 균형상태**로 이동한다는 뜻입니다. **이 모델에 적용할 내용은 가장 낮은 에너지를 가지고 있는 weight과 output값이 가장 안정적인 상태로 보는 것입니다.** 그리고 낮은 에너지일수록 높은 상관관계를 가지고 있는 변수라는 것을 의미하죠. \n",
        "\n",
        "![RBM energy](4.png)\n",
        "\n",
        "위 그림을 보면 에너지가 낮을수록 높은 확률을 가지고 있습니다. 확률분포그래프로 표현하면 다음과 같습니다.\n",
        "\n",
        "![RBM probability](5.png)\n",
        "\n",
        "두 그래프의 차이는 아래에서 설명하겠습니다.\n",
        "\n",
        "\n",
        "## RBM (Restriced Boltzmann Machines)\n",
        "우리는 지금 본격적인 학습을 위한 모델이 아닌 **weight을 먼저 학습시키기 위한 선행작업**을 하고 있음을 짚고 이어가겠습니다. RBM은 주로 비지도 학습에서 사용되고 특히 `Generating Model`로서 활용됩니다. Weight값과 output을 모르는 상황에서 학습을 시키는데 사용되고 있죠. RBM의 학습과정은 다음과 같습니다.\n",
        "\n",
        "![RBM](3.png)\n",
        "\n",
        "Visible layer에서 weight 값이 적용되고 hidden layer (두개의 layer이므로 이 값이 active function을 지나면 output으로 봐도 무방)로 이동합니다. 여기까지는 다른 것들과 동일하지만 Unsupervised Learning은 `재구성`이라는 과정을 거칩니다.\n",
        "\n",
        "![RBM Reconstruction](6.png)\n",
        "\n",
        "앞선 weight와 같은 weight를 사용해서 반대로 이동합니다. Input값에 weight을 부여하고 hidden layer의 label들을 뽑아냅니다. 그 이후에는 반대로 hidden layer의 label에 전에 사용한 weight을 그대로 다시 부여해서 최초 input값과 비교합니다. 학습하기 전의 weight 값은 임의로 부여합니다. RBM은 **output 값을 input값과 가까워지도록 학습하는 것입니다.**\n",
        "위 두개의 과정은 위에서 설명한 확률분포그래프로 표현할 수 있습니다.\n",
        "\n",
        "![RBM probability](5.png)\n",
        "\n",
        "**이 두그래프가 일치하도록 움직이는 것을 `최적화`라고 합니다.** 두 weight을 맞춰주는 과정이죠. 이렇게 input값에 weight을 부여했더니 input값과 일치하는 값이 나왔다면 그 weight는 input값의 특징을 잘 뽑아낸 것이라고 볼 수 있습니다. **우리는 지금 input값의 특징들을 잘 표현한 weight값을 이용해 본격적인 학습을 시작하는 것이 목적이지만, 이런 과정으로 Unsupervised Learning이 진행됩니다.** \n",
        "\n",
        "예를들어, 어떤 사람의 사진을 보여주고 학습을 통해 기계가 비슷한 사람의 사진을 그린다고 해보죠. 그 기계는 input 값으로 들어오는 사람의 특징 (긴 머리카락, 큰 키, 높은 콧대 등등..)을 잘 파악한 것이라고 볼 수 있죠.\n",
        "\n",
        "이렇게 학습된 weight를 가지고 본격적인 학습을 시작합니다. 이 이유 때문에 위와 같은 과정을 `pre-training`이라고도 합니다. 그리고 좋은 weight를 설정하는 이 모든 과정을 `Fine Tuning`이라고 합니다. 그리고 이 RBM을 여러층으로 쌓은 것을 `Deep Belief Network`라고 합니다.\n",
        "\n",
        "*(참고 https://m.blog.naver.com/PostView.nhn?blogId=krlepton&logNo=220699374059&proxyReferer=https%3A%2F%2Fwww.google.com%2F)*"
      ]
    }
  ]
}