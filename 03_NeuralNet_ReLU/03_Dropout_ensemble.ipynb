{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Dropout_ensemble.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsstar522/hunkim_ML/blob/master/03_NeuralNet_ReLU/03_Dropout_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elLdx80cvFi_",
        "colab_type": "text"
      },
      "source": [
        "# Overfitting의 문제\n",
        "\n",
        "Overfitting은 training data에서만 accuracy가 좋고 실제 testing data에서는 accuracy가 떨어지는 현상을 말합니다. 깊은 Layer를 사용할 수록 overfitting이 생겨나는데, 많은 weight로 인해 너무 training data에만 잘 맞게 설정이 된 것이죠. 즉, **너무 많은 변수들을 고려하면 비슷한 변수들을 가진 testing data를 아예 처리하지 못하는 상황이 될 수 있습니다.** Overfitting은 **많은 training data set을 사용하거나 Regularization을 하면 해결할 수 있습니다.**\n",
        "\n",
        "## Regularization\n",
        "\n",
        "많은 weight으로 인해 hypothesis가 너무 구불구불한 형태를 가지고 있다면 Regularization(정규화)을 거쳐야 합니다. 기술적으로 많이 사용하는 정규화 방법에는 `L2 Regularization`이 있습니다. Weight값이 커서 발생하는 overfitting에서 **패널티**를 줌으로써 overfitting을 방지하는 방법입니다. Cost값에 다음과 같은 값을 주면 패널티를 줄 수 있습니다. \n",
        "\n",
        "$$cost + λW^{2}$$\n",
        "\n",
        "λ를 0으로 두면 패널티를 주지 않겠다는 의미이고, 값이 클 수록 많은 패널티를 부여한다는 뜻입니다. 학습은 Cost값을 weight에 대해 미분한 값을 통해 cost가 최소인 부분을 찾아갑니다. overfitting된 hypothesis는 구불구불한 선으로 나옵니다. 구불구불한 선은 고차항의 함수로 표현됩니다. hypothesis가 다음과 같다고 해보겠습니다.\n",
        "\n",
        "$$h(x) = 𝜽_0 + 𝜽_1x + 𝜽_2x^2 + 𝜽_3x^3 + 𝜽_4x^4$$\n",
        "\n",
        "고차항을 없애면 그래프는 부드러워 집니다. 이 hypothesis가 부드러워지도록 하는 방법을 생각해보죠. 위의 cost function은 다음과 같습니다.\n",
        "\n",
        "$$\\frac{1}{m}\\ Σ_i^m(h(x_i) - y_i)^2$$\n",
        "\n",
        "그리고 이 함수가 최소값을 갖는 𝜽 (weight) 값을 찾도록 움직입니다. 즉, 𝜽에 대해 미분한 값이 0이 되는 방향으로 움직이죠. 이제 이 cost function에 다음과 같은 항을 넣어봅시다.\n",
        "\n",
        "$$\\frac{1}{m}\\ Σ_i^m(h(x_i) - y_i)^2 + λ𝜽_3^2 + λ𝜽_4^2$$\n",
        "\n",
        "cost 값은 가장 작은 값이 되도록 움직이므로 $𝜽_3$과 $𝜽_4$는 0에 가깝게 움직일 것입니다. 그렇게 되면 고차항은 없어지므로 부드러운 곡선으로 hypothesis가 생깁니다. \n",
        "\n",
        "## Dropout\n",
        "\n",
        "많은 perceptron(node)이 있는 layer가 deep하게 있는 경우를 생각해보겠습니다. 이 perceptron들은 특징(feature)를 나타냅니다. Overfitting이 생겨나는 이유 중 하나가 너무 많은 특징을 고려해서 다른 case를 학습하지 못하는 경우입니다. 그래서 의도적으로 perceptron(node)를 꺼버리는 것을 `Dropout`이라고 합니다. 사용법은 간단합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z7erOgrNpLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_rate = tf.placeholder('float')\n",
        "_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
        "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
        "\n",
        "## Training\n",
        "sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})\n",
        "## Evaluation\n",
        "accuracy.eval({X: mnist.test.images, Y: mnist.test.labels, dropout_rate: 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HxZAVDk6Awy",
        "colab_type": "text"
      },
      "source": [
        "`dropout_rate`를 0.7로 두면 70%만 사용하겠다는 의미입니다. **Overfitting은 학습시킬 때 생겨나므로 training 할 때만 `dropout_rate`를 적용시키고 실제로 test 할 때는 dropout을 꺼야 합니다.** `dropout_rate`를 1로 두면 끌 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaMNZgQF5rOH",
        "colab_type": "text"
      },
      "source": [
        "## Ensemble\n",
        "\n",
        "`Ensemble`은 독립된 모델 여러개를 사용해서 학습을 시킨 뒤 그 결과를 모으는 방법입니다. 모델 여러개가 다양한 예측값을 내놓고 이 값들을 종합해서 최종 예측값을 내놓습니다. 이 방식으로 학습을 시키면 정확도가 4~5% 정도 향상됩니다.\n",
        "\n",
        "![Ensemble](7.png)\n"
      ]
    }
  ]
}