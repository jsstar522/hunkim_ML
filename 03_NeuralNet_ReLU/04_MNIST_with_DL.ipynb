{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_MNIST_with_DL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsstar522/hunkim_ML/blob/master/03_NeuralNet_ReLU/04_MNIST_with_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFqA2vkDDRq6",
        "colab_type": "text"
      },
      "source": [
        "# MNIST with Deep Learning\n",
        "\n",
        "이제 Softmax classification로 MNIST 예측을 했던 문제를 Deep Learning을 통해 해결해보겠습니다.\n",
        "\n",
        "## Softmax\n",
        "먼저 Softmax를 통한 정확도를 뽑아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFkqoKXYFyD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8b8c69d1-b958-40f0-b864-b95c2be75159"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "## MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"Data/MNIST_data/\", one_hot=True)\n",
        "\n",
        "## 28x28\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "## 0~9\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "W = tf.Variable(tf.random_normal([784, 10]))\n",
        "b = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "## hypothesis\n",
        "logits = tf.matmul(X, W) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "## cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
        "\n",
        "## minimize cost\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "## initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "## accuracy\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "\n",
        "## set epochs & batch size\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    iteration = int(mnist.train.num_examples/batch_size)\n",
        "    ## 전체 데이터에서 batch_size를 나눈 iteration\n",
        "    for i in range(iteration):\n",
        "        ## training dataset\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        avg_cost += c/iteration\n",
        "        \n",
        "    print(\"Epoch:\", \"%04d\" %(epoch + 1), \"Cost: \", \"{:.9f}\" .format(avg_cost))\n",
        "\n",
        "## testing dataset\n",
        "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Cost:  4.655570388\n",
            "Epoch: 0002 Cost:  1.583994455\n",
            "Epoch: 0003 Cost:  1.069022316\n",
            "Epoch: 0004 Cost:  0.854501970\n",
            "Epoch: 0005 Cost:  0.734014441\n",
            "Epoch: 0006 Cost:  0.656466654\n",
            "Epoch: 0007 Cost:  0.600799085\n",
            "Epoch: 0008 Cost:  0.558425213\n",
            "Epoch: 0009 Cost:  0.525775306\n",
            "Epoch: 0010 Cost:  0.498680057\n",
            "Accuracy:  0.886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az1wGXnnNTck",
        "colab_type": "text"
      },
      "source": [
        "## Nueral Network\n",
        "\n",
        "이제 일반적으로 사용하는 multi-layer 형태의 Neural Network를 사용해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBycok_hODWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2a6bc5a8-a650-4e20-92b2-4ac21922df04"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "## MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"Data/MNIST_data/\", one_hot=True)\n",
        "\n",
        "## 28x28\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "## 0~9\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "## layer 1\n",
        "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "## layer 2\n",
        "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "## layer 3\n",
        "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "## cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
        "\n",
        "## minimize cost\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "## initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "## accuracy\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "\n",
        "## set epochs & batch size\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    iteration = int(mnist.train.num_examples/batch_size)\n",
        "    ## 전체 데이터에서 batch_size를 나눈 iteration\n",
        "    for i in range(iteration):\n",
        "        ## training dataset\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        avg_cost += c/iteration\n",
        "        \n",
        "    print(\"Epoch:\", \"%04d\" %(epoch + 1), \"Cost: \", \"{:.9f}\" .format(avg_cost))\n",
        "\n",
        "## testing dataset\n",
        "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Cost:  195.752253359\n",
            "Epoch: 0002 Cost:  44.764822620\n",
            "Epoch: 0003 Cost:  27.828198097\n",
            "Epoch: 0004 Cost:  19.627305823\n",
            "Epoch: 0005 Cost:  14.092954263\n",
            "Epoch: 0006 Cost:  10.608012293\n",
            "Epoch: 0007 Cost:  8.096181757\n",
            "Epoch: 0008 Cost:  6.040426032\n",
            "Epoch: 0009 Cost:  4.424856464\n",
            "Epoch: 0010 Cost:  3.443676003\n",
            "Accuracy:  0.9429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYFiAC-PQxB",
        "colab_type": "text"
      },
      "source": [
        "## Xavier initialization\n",
        "\n",
        "이번에는 Weight 초기화를 통해 정확도를 높여보겠습니다. Weight 값을 설정할 때, `tf.contrib.layers.xavier_initializer()`를 사용하면 `Xavier 초기화`를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZAHxOcPawb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "72ba51ce-d2f7-440b-b88f-aa5822957525"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "## MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"Data/MNIST_data/\", one_hot=True)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "## 28x28\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "## 0~9\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "## layer 1\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "## layer 2\n",
        "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "## layer 3\n",
        "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "## cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
        "\n",
        "## minimize cost\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "## initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "## accuracy\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "\n",
        "## set epochs & batch size\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    iteration = int(mnist.train.num_examples/batch_size)\n",
        "    ## 전체 데이터에서 batch_size를 나눈 iteration\n",
        "    for i in range(iteration):\n",
        "        ## training dataset\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        avg_cost += c/iteration\n",
        "        \n",
        "    print(\"Epoch:\", \"%04d\" %(epoch + 1), \"Cost: \", \"{:.9f}\" .format(avg_cost))\n",
        "\n",
        "## testing dataset\n",
        "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Cost:  0.310727648\n",
            "Epoch: 0002 Cost:  0.113794084\n",
            "Epoch: 0003 Cost:  0.076681319\n",
            "Epoch: 0004 Cost:  0.055887896\n",
            "Epoch: 0005 Cost:  0.040908583\n",
            "Epoch: 0006 Cost:  0.030830162\n",
            "Epoch: 0007 Cost:  0.023312608\n",
            "Epoch: 0008 Cost:  0.018209115\n",
            "Epoch: 0009 Cost:  0.017060772\n",
            "Epoch: 0010 Cost:  0.014571132\n",
            "Accuracy:  0.9808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02V28zfkdlYX",
        "colab_type": "text"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "먼저 layer를 더 깊게 만들고 node를 더 많이 생성(wide)해서 학습을 시켜보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JA1thQhfGGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "078bb84f-621c-469b-f910-5b2a0983b047"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "## MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"Data/MNIST_data/\", one_hot=True)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "## 28x28\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "## 0~9\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "## layer 1\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "## layer 2\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "## layer 3\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "## layer 4\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "\n",
        "## layer 5\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "## cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
        "\n",
        "## minimize cost\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "## initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "## accuracy\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "\n",
        "## set epochs & batch size\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    iteration = int(mnist.train.num_examples/batch_size)\n",
        "    ## 전체 데이터에서 batch_size를 나눈 iteration\n",
        "    for i in range(iteration):\n",
        "        ## training dataset\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "        avg_cost += c/iteration\n",
        "        \n",
        "    print(\"Epoch:\", \"%04d\" %(epoch + 1), \"Cost: \", \"{:.9f}\" .format(avg_cost))\n",
        "\n",
        "## testing dataset\n",
        "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Cost:  0.312850165\n",
            "Epoch: 0002 Cost:  0.103657596\n",
            "Epoch: 0003 Cost:  0.070952105\n",
            "Epoch: 0004 Cost:  0.053882102\n",
            "Epoch: 0005 Cost:  0.041925784\n",
            "Epoch: 0006 Cost:  0.036304727\n",
            "Epoch: 0007 Cost:  0.027852293\n",
            "Epoch: 0008 Cost:  0.026560210\n",
            "Epoch: 0009 Cost:  0.025539212\n",
            "Epoch: 0010 Cost:  0.021148856\n",
            "Accuracy:  0.9767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYHbH-uRgeVC",
        "colab_type": "text"
      },
      "source": [
        "더 Deep하고 Wide하게 학습을 했음에도 불구하고 결과가 비슷하거나 더 나빠지는 것을 확인할 수 있습니다. 이렇게 Deep하고 Wide 해지면 Overfitting이 발생했을 확률이 굉장히 높습니다. 변수가 굉장히 많기 때문에 testing data에 제대로 적용시키지 못하는 것이죠. 이 문제를 해결하기 위해 `Dropout`을 사용해보겠습니다. Dropout은 임의의 node를 끄는 방법입니다. layer마다 dropout 함수를 적용시켜줍니다. 이 때 몇퍼센트의 node를 사용할 것인지 정하는 것이 dropout 함수의 두번째 인자, keep_prob에 들어가게 되는데 **학습할 때는 대부분 70%, 실제 사용시에는 100%를 사용해야하므로 사전에 keep_prob를 placeholder로 지정(변수)한 뒤 학습과 실전에서 feed_dict에서 값을 넣어 사용**하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQw2N5Rgg1De",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "82d55709-e6b3-4a22-9a7a-e66ae3f95730"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "## MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"Data/MNIST_data/\", one_hot=True)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "## dropout probabilty\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "## 28x28\n",
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "## 0~9\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "## layer 1\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "## keep_prob\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "## layer 2\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "## layer 3\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "## layer 4\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "## layer 5\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "## cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
        "\n",
        "## minimize cost\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "## initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "## accuracy\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "\n",
        "## set epochs & batch size\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    iteration = int(mnist.train.num_examples/batch_size)\n",
        "    ## 전체 데이터에서 batch_size를 나눈 iteration\n",
        "    for i in range(iteration):\n",
        "        ## training dataset\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "        avg_cost += c/iteration\n",
        "        \n",
        "    print(\"Epoch:\", \"%04d\" %(epoch + 1), \"Cost: \", \"{:.9f}\" .format(avg_cost))\n",
        "\n",
        "## testing dataset\n",
        "print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting Data/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting Data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 Cost:  0.456425124\n",
            "Epoch: 0002 Cost:  0.172553552\n",
            "Epoch: 0003 Cost:  0.128965483\n",
            "Epoch: 0004 Cost:  0.112313464\n",
            "Epoch: 0005 Cost:  0.094356168\n",
            "Epoch: 0006 Cost:  0.084927326\n",
            "Epoch: 0007 Cost:  0.077305277\n",
            "Epoch: 0008 Cost:  0.069215620\n",
            "Epoch: 0009 Cost:  0.064746225\n",
            "Epoch: 0010 Cost:  0.058119157\n",
            "Accuracy:  0.9831\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}