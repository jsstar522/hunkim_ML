{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Backpropagation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsstar522/hunkim_ML/blob/master/02_NeuralNet_Backpropagation/01_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAAk_WEQJlM7",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt20FNDGJxtm",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network에서 계산량 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arm1j4zgJ8Mx",
        "colab_type": "text"
      },
      "source": [
        "Neural Network는 수많은 Layer와 노드(perceptron)으로 구성됩니다. 784개의 데이터를 Input layer에 집어넣고 10개 노드를 가진 1st hidden layer, 10개의 노드를 가진 2nd hidden layer가 있을 때, 필요한 weight와 bias는 몇개이며 계산량은 얼만큼 될까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlikVUsmLzHK",
        "colab_type": "text"
      },
      "source": [
        "![이미지](1.jpg)\n",
        "*[그림1]하나의 노드에는 Input data만큼의 w(Weight)와 하나의 bias가 필요하다*\n",
        "\n",
        "**이렇게 Neural Network는 엄청나게 많은 양의 가중치(Weight)가 필요합니다.** 계산량도 어마어마해질 뿐만 아니라 단순한 Machine learning에서 도출된 Cost Function (Error)처럼 식을 구하는 것조차 힘듭니다. Hypothesis를 통해 나온 1차 측정값 (representation)을 다시 새로운 2차 Hypothesis에 넣어야 하므로 엄청나게 복잡한 식이 나오겠죠. 이러한 복잡한 식으로 Error를 구하기는 불가능하다는 것을 *Marvin Minsky* 박사가 1969년에 증명했습니다. 이런 복잡한 식으로 미분을 해서 최소값을 찾아가는 과정...!@$%# 말이 안되죠. 이렇게 인공지능의 침체기가 시작됐지만 Backpropagation이 등장하면서 다시 불이 붙기 시작하죠.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zujdlCPscDF7",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQp21HqmdTgP",
        "colab_type": "text"
      },
      "source": [
        "**Backpropagation은 중간에 계산과정이 어떻게 됐던지 간에 오차가 줄어드는 방향을 근거 있게 설명합니다. 예측값이 나오면 Error을 계산하고 오류에 영향을 가장 크게 미치는 Weight값을 찾습니다. 그리고 가중치(Weight)를 Update 하죠.** \n",
        "\n",
        "수많은 데이터가 수많은 다음 layer의 노드로 이동하는 과정 중에 하나만 확대해서 그려보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsMmSIBoemqm",
        "colab_type": "text"
      },
      "source": [
        "![이미지](2.jpg)\n",
        "*[그림2]하나의 input data가 하나의 노드로 이동하는 과정*\n",
        "\n",
        "이렇게 예측값이 나왔을 때 w(weight)가 예측값(f)에 미치는 영향이 얼마나 되는지 계산해보죠. **얼마나 영향을 미치는지 알아보려면 w값이 변할 때, f값은 얼마나 변하는지 구하면 됩니다. 즉, f를 w에 대해 편미분하면 되죠.**\n",
        "\n",
        "$$\\frac{af}{aw}$$\n",
        "\n",
        "f = g + b이므로,\n",
        "\n",
        "$$\\frac{af}{ag}\\frac{ag}{aw}$$\n",
        "\n",
        "로 표현되어야 합니다. (이걸 미적분학에서 **Chain Rule**이라고 합니다.)\n",
        "\n",
        "위 그림에서,\n",
        "$$\\frac{af}{ag} = 1,\\frac{ag}{aw} = x_{(x=5)}$$\n",
        "\n",
        "이므로\n",
        "\n",
        "$$\\frac{af}{aw} = 5$$\n",
        "\n",
        "가 됩니다.\n",
        "\n",
        "이 의미는 w가 1증가 할 때, f가 5배만큼 증가한다는 뜻이죠. w가 큰 영향을 미치는군요.\n",
        "\n",
        "**같은 방식으로 f를 Error로 바꿔서 풀면 Error에 큰 영향을 미치는 w(weight)를 찾을 수 있게 됩니다. 그렇게 w를 \"Optimizing\" 해나가는 것입니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiG-5FiSjJGB",
        "colab_type": "text"
      },
      "source": [
        "![이미지](3.jpg)\n",
        "*[그림3] 거꾸로 편미분해 나아가면 최초 w(weight) 값이 최종 f값에 얼마나 영향을 미치는지 알 수 있다.*\n",
        "\n",
        "[그림3]을 보면 왜 Backpropagation이라는 이름이 붙었는지 느낌이 올겁니다. \n",
        "\n",
        "**이렇게 수많은 Layer를 거친 Hypothesis를 구해 복잡하게 미분을 하지 않고, 각 Layer에서의 편미분 값을 이용해 가중치(weight)의 영향력을 알 수 있습니다. 이 방법으로 Error에 미치는 영향력도 알 수 있고 Optimizing 하는 데 더 수월해졌습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUKbiwworVH8",
        "colab_type": "text"
      },
      "source": [
        "## XOR 문제 풀기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rlKUIWwrZlL",
        "colab_type": "text"
      },
      "source": [
        "이제 XOR 문제를 풀어보도록 하겠습니다. 일반적인 Logistic Regression으로 먼저 풀어볼텐데요, 먼저 XOR 데이터를 생성합니다. XOR은 각 숫자가 같을 때는 0, 다를 때는 1을 출력하는 논리구조입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPQaoT9muSh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "387b4a29-ae77-4126-edd6-10087f79229a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "print(x_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz1p3L34vOmZ",
        "colab_type": "text"
      },
      "source": [
        "이제 Hypothesis를 정해보도록 하죠. Weight는 2개의 데이터가 들어가고 하나의 데이터로 나오기 때문에 2 x 1 의 행렬입니다. bias는 출력되는 데이터와 같으므로 1차원 구조입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc1z2-r9vFsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59530bc5-686d-4f77-a499-b1f84b96a1fd"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
        "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
        "\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
        "            \n",
        "    ## x_data, y_data 학습데이터 그대로 테스트\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.2679496 [[-0.40582794]\n",
            " [-1.0018556 ]]\n",
            "100 0.696044 [[ 0.25111973]\n",
            " [-0.09925942]]\n",
            "200 0.69397414 [[ 0.1578951 ]\n",
            " [-0.02948391]]\n",
            "300 0.69343066 [[ 0.09363437]\n",
            " [-0.00649389]]\n",
            "400 0.6932497 [[0.05616169]\n",
            " [0.0026675 ]]\n",
            "500 0.6931863 [[0.03414351]\n",
            " [0.00556552]]\n",
            "600 0.6931629 [[0.02103469]\n",
            " [0.00576785]]\n",
            "700 0.6931537 [[0.01312332]\n",
            " [0.00496756]]\n",
            "800 0.69315004 [[0.00828382]\n",
            " [0.00392691]]\n",
            "900 0.6931484 [[0.00528466]\n",
            " [0.00295716]]\n",
            "1000 0.6931478 [[0.00340315]\n",
            " [0.00215977]]\n",
            "1100 0.6931474 [[0.0022095 ]\n",
            " [0.00154528]]\n",
            "1200 0.6931473 [[0.00144459]\n",
            " [0.00108978]]\n",
            "1300 0.69314724 [[0.00095007]\n",
            " [0.00076053]]\n",
            "1400 0.6931472 [[0.00062788]\n",
            " [0.00052664]]\n",
            "1500 0.6931472 [[0.00041672]\n",
            " [0.00036264]]\n",
            "1600 0.6931472 [[0.00027747]\n",
            " [0.00024858]]\n",
            "1700 0.6931472 [[0.00018523]\n",
            " [0.0001698 ]]\n",
            "1800 0.6931472 [[0.00012393]\n",
            " [0.00011569]]\n",
            "1900 0.6931472 [[8.305711e-05]\n",
            " [7.865582e-05]]\n",
            "2000 0.6931472 [[5.574478e-05]\n",
            " [5.339389e-05]]\n",
            "2100 0.6931472 [[3.7455095e-05]\n",
            " [3.6200930e-05]]\n",
            "2200 0.6931472 [[2.5182489e-05]\n",
            " [2.4519908e-05]]\n",
            "2300 0.6931472 [[1.6945130e-05]\n",
            " [1.6588016e-05]]\n",
            "2400 0.6931472 [[1.140637e-05]\n",
            " [1.122211e-05]]\n",
            "2500 0.6931472 [[7.6736305e-06]\n",
            " [7.5757971e-06]]\n",
            "2600 0.6931472 [[5.158316e-06]\n",
            " [5.111144e-06]]\n",
            "2700 0.6931472 [[3.481931e-06]\n",
            " [3.463073e-06]]\n",
            "2800 0.6931472 [[2.3449725e-06]\n",
            " [2.3350558e-06]]\n",
            "2900 0.6931472 [[1.5805412e-06]\n",
            " [1.5810551e-06]]\n",
            "3000 0.6931472 [[1.0664476e-06]\n",
            " [1.0669615e-06]]\n",
            "3100 0.6931471 [[7.0732864e-07]\n",
            " [7.0784262e-07]]\n",
            "3200 0.6931472 [[4.6443952e-07]\n",
            " [4.6495353e-07]]\n",
            "3300 0.6931472 [[3.1542734e-07]\n",
            " [3.1594135e-07]]\n",
            "3400 0.6931472 [[1.962176e-07]\n",
            " [1.967316e-07]]\n",
            "3500 0.6931472 [[1.4406334e-07]\n",
            " [1.4457734e-07]]\n",
            "3600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "3700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "3800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "3900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "4900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "5900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "6900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "7900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "8900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9100 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9200 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9300 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9400 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9500 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9600 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9700 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9800 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "9900 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "10000 0.6931472 [[1.3214236e-07]\n",
            " [1.3265637e-07]]\n",
            "\n",
            "Hypothesis:  [[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]] \n",
            "Correct:  [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]] \n",
            "Accuracy:  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pbqrwmlxm5C",
        "colab_type": "text"
      },
      "source": [
        "이렇게 일반적인 Logistic Regression으로 학습을 시키고 테스트를 해보면 학습 데이터를 그대로 줬음에도 불구하고 정확도가 50%밖에 되지 않습니다. 즉, XOR 문제를 잘 풀지 못하는거죠. 이제 hidden layer를 넣어서 학습시켜보도록 하겠습니다.\n",
        "\n",
        "[ 0 0 ] ======> [ 2 x 2 행렬 (weight) ] + [ bias ] = [ 1 x 2 행렬 ] =======> [ 2 x 1 행렬 (weight) ] + [ bias ] = 하나의 값\n",
        "\n",
        "의 Multi layer 형태로 학습을 시켜보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBVbcYQ2yIGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a47b20d1-2dbe-489e-bd05-3a1bfb8c9077"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "# W = tf.Variable(tf.random_normal([2, 1]), name=\"weight\")\n",
        "# b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
        "# hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([2, 2]), name=\"wieght1\")\n",
        "b1 = tf.Variable(tf.random_normal([2]), name=\"bias1\")\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([2, 1]), name=\"wieght2\")\n",
        "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for step in range(10001):\n",
        "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
        "        if step % 100 == 0:\n",
        "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
        "            \n",
        "    ## x_data, y_data 학습데이터 그대로 테스트\n",
        "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.9915812 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "100 0.703548 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "200 0.6974315 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "300 0.6929913 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "400 0.68941087 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "500 0.68626773 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "600 0.6832745 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "700 0.6802027 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "800 0.67685133 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "900 0.67302775 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1000 0.6685319 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1100 0.66314125 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1200 0.6565964 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1300 0.64858156 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1400 0.6386995 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1500 0.62643564 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1600 0.61111414 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1700 0.5918702 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1800 0.5677097 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "1900 0.5377759 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2000 0.50185573 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2100 0.46087083 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2200 0.41689625 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2300 0.37258226 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2400 0.33034956 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2500 0.2918424 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2600 0.2578221 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2700 0.22836661 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2800 0.20314963 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "2900 0.18166602 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3000 0.1633724 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3100 0.14775951 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3200 0.13438095 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3300 0.1228598 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3400 0.11288372 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3500 0.10419643 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3600 0.0965887 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3700 0.089889586 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3800 0.0839593 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "3900 0.07868302 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4000 0.07396613 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4100 0.06973016 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4200 0.06590986 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4300 0.06245074 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4400 0.059306867 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4500 0.056439295 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4600 0.05381512 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4700 0.05140611 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4800 0.049188264 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "4900 0.047140624 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5000 0.045245185 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5100 0.043486424 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5200 0.041850604 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5300 0.040325858 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5400 0.038901575 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5500 0.037568633 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5600 0.036318853 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5700 0.035144843 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5800 0.03404023 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "5900 0.032999307 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6000 0.032016784 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6100 0.031088099 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6200 0.030209072 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6300 0.02937594 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6400 0.028585382 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6500 0.0278342 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6600 0.027119689 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6700 0.02643928 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6800 0.02579064 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "6900 0.025171662 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7000 0.024580408 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7100 0.024015155 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7200 0.023474205 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7300 0.02295606 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7400 0.022459408 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7500 0.021982893 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7600 0.021525448 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7700 0.021085873 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7800 0.020663202 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "7900 0.0202565 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8000 0.019864913 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8100 0.019487599 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8200 0.019123847 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8300 0.01877297 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8400 0.01843423 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8500 0.018107139 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8600 0.017790992 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8700 0.017485363 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8800 0.01718966 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "8900 0.016903501 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9000 0.01662641 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9100 0.016357945 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9200 0.016097717 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9300 0.015845342 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9400 0.01560057 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9500 0.015363001 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9600 0.015132331 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9700 0.0149082495 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9800 0.014690481 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "9900 0.014478823 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "10000 0.014273019 [[-1.5572703 ]\n",
            " [-0.56141967]]\n",
            "\n",
            "Hypothesis:  [[0.01744462]\n",
            " [0.9871143 ]\n",
            " [0.9871187 ]\n",
            " [0.01346761]] \n",
            "Correct:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynwym5Ul024a",
        "colab_type": "text"
      },
      "source": [
        "이렇게 Multi layer를 사용하면 XOR문제를 풀 수 있습니다. Hidden layer에서 node(perceptron)을 더 많이 사용할 수도 있습니다. (node를 많이 사용할 수록 \"wide하다\"라고 합니다.) 더 wide하게 사용하면 optimize를 더 효과적으로 할 수 있습니다. 게다가 layer를 더 많이 사용해도 (layer를 많이 사용할수록 \"deep하다\"라고 합니다.) 효과적인 학습을 할 수 있습니다."
      ]
    }
  ]
}